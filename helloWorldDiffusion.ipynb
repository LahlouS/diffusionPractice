{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_swiss_roll as swiss_roll\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A minimalist code to demonstrate Diffusion process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> running on mps\n"
     ]
    }
   ],
   "source": [
    "# UTILS\n",
    "\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print('--> running on', device)\n",
    "\n",
    "def sample_batch(size):\n",
    "    x, _ = swiss_roll(size)\n",
    "    return x[:, [2, 0]] / 10.0 * np.array([1, -1])\n",
    "\n",
    "def plot(model):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    x0 = sample_batch(5000)\n",
    "    # getting the sample at t=20 and t=40\n",
    "    x20 = model.forward_process(torch.from_numpy(x0).to(torch.float32).to(device), 20)[-1].data.cpu().numpy()\n",
    "    x40 = model.forward_process(torch.from_numpy(x0).to(torch.float32).to(device), 40)[-1].data.cpu().numpy()\n",
    "\n",
    "    data = [x0, x20, x40]\n",
    "\n",
    "    for i, t in enumerate([0, 20, 39]):\n",
    "        plt.subplot(2, 3, 1 + i)\n",
    "        plt.scatter(data[i][:, 0], data[i][:, 1], alpha=0.1, s=1)\n",
    "        plt.xlim([-2, 2])\n",
    "        plt.ylim([-2, 2])\n",
    "        plt.gca().set_aspect('equal')\n",
    "        if t == 0: \n",
    "            plt.ylabel(r'$q(\\mathbf{x}^{(0...T)})$', fontsize=17, rotation=0, labelpad=60)\n",
    "        if i == 0: \n",
    "            plt.title(r'$t=0$', fontsize=17)\n",
    "        if i == 1:\n",
    "            plt.title(r'$t=\\frac{T}{2}$', fontsize=17)\n",
    "        if i == 2: \n",
    "            plt.title(r'$t=T$', fontsize=17)\n",
    "    \n",
    "    samples = model.sample(5000, device)\n",
    "    for i, t in enumerate([0, 20, 40]):\n",
    "        plt.subplot(2, 3, 4 + i)\n",
    "        plt.scatter(samples[40 - t][:, 0].data.cpu().numpy(), samples[40 - t][:, 1].data.cpu().numpy(), alpha=.1, s=1, c='r')\n",
    "        plt.xlim([-2, 2])\n",
    "        plt.ylim([-2, 2])\n",
    "        plt.gca().set_aspect('equal')\n",
    "        if t == 0: \n",
    "            plt.ylabel(r'$p(\\mathbf{x}^{(0...T)})$', fontsize=17, rotation=0, labelpad=60)\n",
    "    plt.savefig(\"Imgs/diffusion_model.png\", bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We need a network to learn the conditional distribution q(x_t | x_{t+1})\n",
    "\n",
    "Since our dataset is a 2D (x, y) swiss roll coordinates our neural net will learn to predict the mu and sigma of the reverse gaussain distribution to denoise an input at time Xt to Xt-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, N=40, data_dim=2, hidden_dim=64):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        self.network_head = nn.Sequential(nn.Linear(data_dim, hidden_dim), nn.ReLU(),\n",
    "                                          nn.Linear(hidden_dim, hidden_dim), nn.ReLU(), )\n",
    "        self.network_tail = nn.ModuleList([nn.Sequential(nn.Linear(hidden_dim, hidden_dim),\n",
    "                                                         nn.ReLU(), nn.Linear(hidden_dim, data_dim * 2)\n",
    "                                                         ) for _ in range(N)])\n",
    "\n",
    "    def forward(self, x, t: int):\n",
    "        h = self.network_head(x)\n",
    "        return self.network_tail[t](h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffusionModel(nn.Module):\n",
    "    def __init__(self, mlp_model, T=40, device='mps'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mlp_model = mlp_model\n",
    "        self.T = T\n",
    "        self.device = device\n",
    "\n",
    "        # contruction our non linear betas schedule\n",
    "\n",
    "        betas = torch.linspace(-18, 10, self.T)\n",
    "        self.betas = torch.sigmoid(betas) * (3e-1 - 1e-5) + 1e-5\n",
    "\n",
    "\n",
    "        self.alpha = 1. - self.betas\n",
    "        self.alpha_bar = torch.cumprod(self.alpha, dim=0) # cumulative product\n",
    "        self.sigma2 = self.betas\n",
    "\n",
    "    def forward_process(self, x0, t):\n",
    "\n",
    "        t = t - 1  # Start indexing at 0\n",
    "        beta_forward = self.betas[t]\n",
    "        alpha_forward = self.alpha[t]\n",
    "        alpha_cum_forward = self.alpha_bar[t]\n",
    "        xt = x0 * torch.sqrt(alpha_cum_forward) + torch.randn_like(x0) * torch.sqrt(1. - alpha_cum_forward)\n",
    "        # Retrieved from https://github.com/Sohl-Dickstein/Diffusion-Probabilistic-Models/blob/master/model.py#L203\n",
    "        mu1_scl = torch.sqrt(alpha_cum_forward / alpha_forward)\n",
    "        mu2_scl = 1. / torch.sqrt(alpha_forward)\n",
    "        cov1 = 1. - alpha_cum_forward / alpha_forward\n",
    "        cov2 = beta_forward / alpha_forward\n",
    "        lam = 1. / cov1 + 1. / cov2\n",
    "        mu = (x0 * mu1_scl / cov1 + xt * mu2_scl / cov2) / lam\n",
    "        sigma = torch.sqrt(1. / lam)\n",
    "        return mu, sigma, xt\n",
    "\n",
    "    def reverse(self, xt, t):\n",
    "\n",
    "        t = t - 1  # Start indexing at 0\n",
    "        if t == 0: \n",
    "            return None, None, xt\n",
    "        mu, h = self.mlp_model(xt, t).chunk(2, dim=1)\n",
    "        sigma = torch.sqrt(torch.exp(h))\n",
    "        samples = mu + torch.randn_like(xt) * sigma\n",
    "        return mu, sigma, samples\n",
    "\n",
    "    def sample(self, size, device):\n",
    "        noise = torch.randn((size, 2)).to(device)\n",
    "        samples = [noise]\n",
    "        for t in range(self.T):\n",
    "            _, _, x = self.reverse(samples[-1], self.T - t - 1 + 1)\n",
    "            samples.append(x)\n",
    "        return samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██▏       | 32120/150000 [15:32<59:52, 32.81it/s]  "
     ]
    }
   ],
   "source": [
    "def train(model, optimizer, nb_epochs=150000, batch_size=64_000):\n",
    "    training_loss = []\n",
    "    for _ in tqdm(range(nb_epochs)):\n",
    "        x0 = torch.from_numpy(sample_batch(batch_size)).float().to(device)\n",
    "        t = np.random.randint(2, 40 + 1)\n",
    "        mu_posterior, sigma_posterior, xt = model.forward_process(x0, t)\n",
    "        mu, sigma, _ = model.reverse(xt, t)\n",
    "\n",
    "        KL = (torch.log(sigma) - torch.log(sigma_posterior) + (sigma_posterior ** 2 + (mu_posterior - mu) ** 2) / (\n",
    "                2 * sigma ** 2) - 0.5)\n",
    "        loss = KL.mean()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        training_loss.append(loss.item())\n",
    "\n",
    "\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "model_mlp = MLP(hidden_dim=128).to(device)\n",
    "model = DiffusionModel(model_mlp)\n",
    "optimizer = torch.optim.Adam(model_mlp.parameters(), lr=1e-4)\n",
    "train(model, optimizer)\n",
    "plot(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0005e-05, 1.0009e-05, 1.0019e-05, 1.0039e-05, 1.0081e-05, 1.0166e-05,\n",
      "        1.0339e-05, 1.0696e-05, 1.1426e-05, 1.2924e-05, 1.5995e-05, 2.2291e-05,\n",
      "        3.5199e-05, 6.1659e-05, 1.1589e-04, 2.2702e-04, 4.5461e-04, 9.2014e-04,\n",
      "        1.8701e-03, 3.7989e-03, 7.6763e-03, 1.5317e-02, 2.9796e-02, 5.5312e-02,\n",
      "        9.5000e-02, 1.4616e-01, 1.9823e-01, 2.3992e-01, 2.6735e-01, 2.8313e-01,\n",
      "        2.9153e-01, 2.9581e-01, 2.9794e-01, 2.9899e-01, 2.9951e-01, 2.9976e-01,\n",
      "        2.9988e-01, 2.9994e-01, 2.9997e-01, 2.9999e-01])\n"
     ]
    }
   ],
   "source": [
    "betas = torch.linspace(-18, 10, 40)\n",
    "betas = torch.sigmoid(betas) * (3e-1 - 1e-5) + 1e-5\n",
    "\n",
    "print(betas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "devIA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
